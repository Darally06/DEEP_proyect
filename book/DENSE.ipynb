{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad615a39",
   "metadata": {},
   "source": [
    "# Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5a5dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DirectML device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# ==== Standard Libraries ====\n",
    "import os, time, warnings\n",
    "\n",
    "# ==== Scientific & Data Handling ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== PyTorch Core ====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "# ==== ML & Evaluation ====\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, f1_score,\n",
    "    accuracy_score, balanced_accuracy_score, roc_auc_score, recall_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# ==== Utilities ====\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== Reproducibility ====\n",
    "SEED = 42\n",
    "\n",
    "# ==== Device Handling (DirectML + fallback CPU) ====\n",
    "try:\n",
    "    import torch_directml\n",
    "    DEVICE = torch_directml.device()\n",
    "    print(\"Using DirectML device:\", DEVICE)\n",
    "except Exception as e:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"DirectML no disponible. Usando CPU:\", e)\n",
    "\n",
    "# ==== Warnings ====\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b5ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '../Data/Tabular.csv'\n",
    "FOLDER_PATH = r\"C:\\Users\\usuario\\MRI\\IMAGES_npy\"\n",
    "\n",
    "OUTPUT_DIR = \"../Models_Output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1a68b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes únicos encontrados: {(160, 192, 192)}\n",
      "Total de imágenes: 220\n",
      "Media global promedio: 0.0000\n",
      "Desviación global promedio: 1.0000\n",
      "Size únicos: {5898240}\n"
     ]
    }
   ],
   "source": [
    "# [] Atributos de las imégenes\n",
    "shapes, means, stds, size = [], [], [], []\n",
    "\n",
    "for f in os.listdir(FOLDER_PATH):\n",
    "    if f.endswith(\".npy\"):\n",
    "        img = np.load(os.path.join(FOLDER_PATH, f))\n",
    "        shapes.append(img.shape)\n",
    "        means.append(img.mean())\n",
    "        stds.append(img.std())\n",
    "        size.append(img.size)\n",
    "\n",
    "# Contar shapes únicos\n",
    "shapes_unicos = set(shapes)\n",
    "size_unicos = set(size)\n",
    "print(\"Shapes únicos encontrados:\", shapes_unicos)\n",
    "print(\"Total de imágenes:\", len(shapes))\n",
    "print(f\"Media global promedio: {np.mean(means):.4f}\")\n",
    "print(f\"Desviación global promedio: {np.mean(stds):.4f}\")\n",
    "print(f\"Size únicos: {size_unicos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa76930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CSV\n",
    "# ============================\n",
    "df = pd.read_csv(CSV_PATH, dtype={'sujeto_id': str})\n",
    "df = df.dropna(subset=['is_dementia'])\n",
    "\n",
    "df['sujeto_id'] = df['sujeto_id'].astype(str).str.strip()\n",
    "df['imagen_id'] = df['imagen_id'].astype(str).str.strip()\n",
    "\n",
    "# FULL FILE PATH\n",
    "def build_path(row):\n",
    "    filename = f\"{row['sujeto_id']}_{row['imagen_id']}.npy\"\n",
    "    return os.path.join(FOLDER_PATH, filename)\n",
    "\n",
    "df['file_path'] = df.apply(build_path, axis=1)\n",
    "\n",
    "df = df[['file_path', 'sujeto_id', 'is_dementia']].rename(\n",
    "    columns={'is_dementia': 'label'}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea0b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DISTRIBUCIÓN DE CLASES ===\n",
      "Clase 0 (No Dementia): 158 imágenes - 71.82%\n",
      "Clase 1 (Dementia): 62 imágenes - 28.18%\n",
      "Estadístico Z: -7.1933\n",
      "Valor p: 6.3244e-13\n",
      "\n",
      "Conclusión: Se rechaza H0. La proporción de la Clase 1 (28.18%) es SIGNIFICATIVAMENTE diferente de 50%.\n"
     ]
    }
   ],
   "source": [
    "# 1. Distribución básica de is_dementia\n",
    "\n",
    "class_distribution = df['label'].value_counts().sort_index()\n",
    "print(\"=== DISTRIBUCIÓN DE CLASES ===\")\n",
    "print(f\"Clase 0 (No Dementia): {class_distribution.get(0, 0)} imágenes - {class_distribution.get(0, 0)/len(df)*100:.2f}%\")\n",
    "print(f\"Clase 1 (Dementia): {class_distribution.get(1, 0)} imágenes - {class_distribution.get(1, 0)/len(df)*100:.2f}%\")\n",
    "class_distribution = df['label'].value_counts().sort_index()\n",
    "N = len(df)\n",
    "count_dementia = class_distribution.get(1, 0)\n",
    "expected_proportion = 0.5 \n",
    "z_stat, p_value = proportions_ztest(count=count_dementia, \n",
    "                                    nobs=N, \n",
    "                                    value=expected_proportion, \n",
    "                                    alternative='two-sided')\n",
    "print(f\"Estadístico Z: {z_stat:.4f}\")\n",
    "print(f\"Valor p: {p_value:.4e}\")\n",
    "# Conclusión\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(f\"\\nConclusión: Se rechaza H0. La proporción de la Clase 1 ({count_dementia/N*100:.2f}%) es SIGNIFICATIVAMENTE diferente de {expected_proportion*100:.0f}%.\")\n",
    "else:\n",
    "    print(f\"\\nConclusión: No se rechaza H0. No hay evidencia suficiente para decir que la proporción es diferente de {expected_proportion*100:.0f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2511e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_stratified_split(records):\n",
    "    \"\"\"\n",
    "    Split estratificado por SUJETO:\n",
    "        Train 60%\n",
    "        Val   20%\n",
    "        Test  20%\n",
    "    Sin fuga de información.\n",
    "    \"\"\"\n",
    "\n",
    "    df_local = pd.DataFrame(records)\n",
    "\n",
    "    # 1. Etiqueta por sujeto (promedio -> entero)\n",
    "    subj_lab = df_local.groupby(\"sujeto_id\")[\"label\"].agg(lambda x: int(round(x.mean())))\n",
    "    subjects = subj_lab.index.to_list()\n",
    "    y = subj_lab.values\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Train (60%) vs Resto (40%)\n",
    "    # -----------------------------\n",
    "    gss1 = GroupShuffleSplit(n_splits=1, train_size=0.6, random_state=SEED)\n",
    "    train_idx, rest_idx = next(gss1.split(subjects, y, groups=subjects))\n",
    "\n",
    "    train_subj = [subjects[i] for i in train_idx]\n",
    "    rest_subj  = [subjects[i] for i in rest_idx]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Val (20%) vs Test (20%) dentro del 40%\n",
    "    # -----------------------------\n",
    "    rest_labels = [subj_lab[s] for s in rest_subj]\n",
    "\n",
    "    val_subj, test_subj = train_test_split(\n",
    "        rest_subj,\n",
    "        test_size=0.5,\n",
    "        random_state=SEED,\n",
    "        stratify=rest_labels\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Regresar DataFrames\n",
    "    # -----------------------------\n",
    "    def pick(subjects_list):\n",
    "        return df_local[df_local[\"sujeto_id\"].isin(subjects_list)].reset_index(drop=True)\n",
    "\n",
    "    return pick(train_subj), pick(val_subj), pick(test_subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6342a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PASO 2] Construcción del DataSet MRI 2.5D\n",
    "# ---------------------------\n",
    "\n",
    "class MRI2p5DDataset(Dataset):\n",
    "    def __init__(self, df, n_slices, target_size=(224,224), augment=False):\n",
    "        \"\"\"\n",
    "        df: DataFrame con columnas 'file_path' y 'label'\n",
    "        n_slices: número de cortes a tomar por volumen\n",
    "        target_size: tamaño HxW de las imágenes\n",
    "        augment: aplicar data augmentation\n",
    "        \"\"\"\n",
    "        self.records = df.to_dict(\"records\")\n",
    "        self.n_slices = n_slices\n",
    "        self.target_size = target_size\n",
    "        self.augment = augment\n",
    "\n",
    "        # Transforms\n",
    "        self.augment_tf = T.Compose([\n",
    "            T.RandomRotation(10),\n",
    "            T.RandomResizedCrop(target_size, scale=(0.9,1.0))\n",
    "        ])\n",
    "        self.base_tf = T.Compose([\n",
    "            T.Resize(target_size),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        vol = np.load(rec[\"file_path\"])  # (D,H,W) 3D array, Cortes-Altura-Ancho\n",
    "\n",
    "        # Elegir n_slices uniformemente\n",
    "        D = vol.shape[0]\n",
    "        idxs = np.linspace(0, D-1, self.n_slices).astype(int)\n",
    "        slices = vol[idxs]  # (n_slices,H,W)\n",
    "\n",
    "        # Convertir cada slice a tensor\n",
    "        imgs = []\n",
    "        for s in slices:\n",
    "            # Normalizar a [0,255] y convertir a PIL\n",
    "            s_img = ((s - s.min())/(s.max()-s.min()+1e-6) * 255).astype(np.uint8)\n",
    "            img = Image.fromarray(s_img).convert(\"L\")  # 1 canal\n",
    "            if self.augment:\n",
    "                img = self.augment_tf(img)\n",
    "            img = self.base_tf(img)  # (1,H,W)\n",
    "            imgs.append(img)\n",
    "        \n",
    "        # Apilar slices como canales -> (n_slices,H,W)\n",
    "        input_tensor = torch.cat(imgs, dim=0)\n",
    "        \n",
    "        label = torch.tensor(rec[\"label\"], dtype=torch.float32)\n",
    "        return input_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01907810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PASO 3] Modelo ResNet18 MRI 2.5D\n",
    "# ---------------------------\n",
    "\n",
    "class DenseNet121_2p5D(nn.Module):\n",
    "    def __init__(self, n_slices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = densenet121(weights=None)\n",
    "\n",
    "        # Modificar la primera capa para aceptar n_slices\n",
    "        self.model.features.conv0 = nn.Conv2d(\n",
    "            in_channels=n_slices,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # Clasificador final a 1 logit\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 1)   # DenseNet-121 output size = 1024\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686e10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PASO 4] Entrenamiento por epoca y evaluacion\n",
    "# ---------------------------\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()                    # Coloca el modelo en modo entrenamiento\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for X, y in pbar:\n",
    "        X, y = X.to(device), y.to(device)  # Enviar batch a GPU/CPU\n",
    "        optimizer.zero_grad()              # Reiniciar gradientes\n",
    "\n",
    "        logits = model(X)                  # Forward\n",
    "        loss = criterion(logits, y)        # Calcular pérdida\n",
    "        loss.backward()                     # Backpropagation\n",
    "        optimizer.step()                    # Actualizar pesos\n",
    "\n",
    "        total_loss += loss.item() * y.size(0)  # Acumular pérdida ponderada por batch\n",
    "        total += y.size(0)\n",
    "        pbar.set_postfix({\"batch_loss\": loss.item()})  # Mostrar pérdida por batch\n",
    "\n",
    "    return total_loss / total  # Pérdida promedio por sample\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "            pred = (prob >= 0.5).astype(int)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(pred)\n",
    "            all_probs.extend(prob)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "\n",
    "    return acc, bal_acc, auc, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a190e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(df, batch_size, n_slices, target_size=(224,224)):\n",
    "    records = df.to_dict(\"records\")\n",
    "\n",
    "    # -------- SPLIT POR SUJETO (60/20/20) --------\n",
    "    train_df, val_df, test_df = group_stratified_split(records)\n",
    "\n",
    "    #print(f\"Dimensiones: Train {len(train_df)} | Val {len(val_df)} | Test {len(test_df)}\")\n",
    "\n",
    "    # ========== CALCULAR CLASE PREDOMINANTE POR SUJETO ==========\n",
    "    subj_lab = train_df.groupby(\"sujeto_id\")[\"label\"].agg(lambda x: int(round(x.mean())))\n",
    "\n",
    "    #print(\"\\n=== Etiqueta predominante por sujeto (Train) ===\")\n",
    "    #print(subj_lab.value_counts())\n",
    "\n",
    "    # Sujetos mayoritarios/minoritarios según la etiqueta predominante\n",
    "    maj_subj = subj_lab[subj_lab == 0].index.tolist()\n",
    "    min_subj = subj_lab[subj_lab == 1].index.tolist()\n",
    "\n",
    "    #print(f\"Sujetos clase 0 (majority): {len(maj_subj)}\")\n",
    "    #print(f\"Subjects clase 1 (minority): {len(min_subj)}\")\n",
    "\n",
    "    # ========== UPSAMPLING A NIVEL DE SUJETO ==========\n",
    "    min_subj_upsampled = resample(\n",
    "        min_subj,\n",
    "        replace=True,\n",
    "        n_samples=len(maj_subj),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    #print(\"\\nSujetos repetidos en oversampling:\")\n",
    "    #print(pd.Series(min_subj_upsampled).value_counts())\n",
    "\n",
    "    # ========== RECONSTRUIR TRAIN BALANCEADO SIN MODIFICAR VISITAS ==========\n",
    "    train_df_bal = pd.concat([\n",
    "        train_df[train_df[\"sujeto_id\"].isin(maj_subj)],\n",
    "        train_df[train_df[\"sujeto_id\"].isin(min_subj_upsampled)]\n",
    "    ]).sample(frac=1, random_state=42)\n",
    "\n",
    "    #print(\"\\n=== Distribución después de oversampling (por imagen) ===\")\n",
    "    #print(train_df_bal[\"label\"].value_counts())\n",
    "\n",
    "    # ========== DATASETS ==========\n",
    "    train_dataset = MRI2p5DDataset(train_df_bal, n_slices=n_slices,\n",
    "                                   target_size=target_size, augment=True)\n",
    "    val_dataset   = MRI2p5DDataset(val_df, n_slices=n_slices,\n",
    "                                   target_size=target_size, augment=False)\n",
    "    test_dataset  = MRI2p5DDataset(test_df, n_slices=n_slices,\n",
    "                                   target_size=target_size, augment=False)\n",
    "\n",
    "    # ========== DATALOADERS ==========\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    #print(f\"\\nTrain Loader: {len(train_loader.dataset)} ejemplos, {len(train_loader)} batches\")\n",
    "    #print(f\"Validation Loader: {len(val_loader.dataset)} ejemplos, {len(val_loader)} batches\")\n",
    "    #print(f\"Test Loader: {len(test_loader.dataset)} ejemplos, {len(test_loader)} batches\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68270ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PASO 6] Modulo de entrenamiento\n",
    "# ---------------------------\n",
    "def train_model(\n",
    "        model, train_loader, val_loader, device, \n",
    "        epochs, lr, weight_decay, early_stopping_patience, model_ej):\n",
    "    \"\"\"\n",
    "    Entrena el modelo y retorna listas de pérdidas y métricas.\n",
    "    \"\"\"\n",
    "    print(f\"\\n⏺️ Épocas {epochs} | LR-WD {lr}-{ weight_decay} | Patience {early_stopping_patience}\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    train_losses, val_recalls, val_aucs, val_bac = [], [], [], []\n",
    "    best_bal_acc = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"▶️ Start train\")\n",
    "    time_all = time.time()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        time_ep = time.time()\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        time_train = time.time() - time_ep\n",
    "        acc, bal_acc, auc, recall = evaluate(model, val_loader, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_recalls.append(recall)\n",
    "        val_aucs.append(auc)\n",
    "        val_bac.append(bal_acc)\n",
    "\n",
    "        #print(f\"Epoch {ep+1}/{epochs} | TrainLoss={train_loss:.4f} ||\"\n",
    "           #   f\"BAC ={bal_acc:.4f} | Recall ={recall:.4f} | \"\n",
    "           #   f\"AUC={auc:.4f} | Time={time_train/60:.2f}\")\n",
    "        # Scheduler\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        # EARLY STOPPING usando RECALL DE LA CLASE 1\n",
    "        if bal_acc > best_bal_acc:  # Considera ambas clases\n",
    "            best_bal_acc = bal_acc\n",
    "            patience_counter = 0\n",
    "            model_path = f\"../Models_Output/{model_ej}_{epochs}_{ep}_{early_stopping_patience}.pth\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\">> Mejor modelo guardado\"\n",
    "                  f\"Epoch {ep+1}/{epochs} | TrainLoss={train_loss:.4f} ||\"\n",
    "                  f\"BAC ={bal_acc:.4f} | Recall ={recall:.4f} | \"\n",
    "                  f\"AUC={auc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\">> Early stopping activado {ep}\")\n",
    "                break\n",
    "\n",
    "    \n",
    "    print(f\"Tiempo total de entrenamiento: {(time.time() - time_all)/60:.2f} minutos\")\n",
    "    return train_losses, val_bac, val_aucs, model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2f3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final(model, test_loader, device, model_path):\n",
    "    \"\"\"\n",
    "    Evalúa el mejor modelo guardado y calcula métricas relevantes,\n",
    "    incluyendo sensibilidad (recall de clase positiva) y especificidad.\n",
    "    \"\"\"\n",
    "    print(f\"Best model: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    acc = (all_preds == all_labels).float().mean().item()\n",
    "    bal_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"\\n TEST =====\")\n",
    "\n",
    "    print(f\"BAC = {bal_acc:.4f} | AUC = {auc:.4f} | recall = {recall:.4f}  | f1 = {f1:.4f}\")\n",
    "    # Matriz de confusión\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"\\nMatriz de confusión:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Reporte de clasificación\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"No Dementia\", \"Dementia\"])\n",
    "    print(\"\\nReporte de clasificación:\")\n",
    "    print(report)\n",
    "\n",
    "    return acc, bal_acc, auc, recall, f1, cm, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a5a66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main(df, device, n_slices, EPOCHS, LR, WEIGHT, EARLY, BATCH, MODELO):\n",
    "    print(F\"✳️ Entrenando modelo {MODELO}\")\n",
    "    train_loader, val_loader, test_loader = prepare_dataloaders(df, BATCH, n_slices)\n",
    "\n",
    "    model = DenseNet121_2p5D(n_slices).to(device)\n",
    "    \n",
    "    train_losses, val_bac, val_aucs, model_path = train_model(\n",
    "        model, train_loader, val_loader, device, \n",
    "        EPOCHS, LR, WEIGHT, EARLY, MODELO)\n",
    "        \n",
    "    # Evaluación final\n",
    "    evaluate_final(model, test_loader, device, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a682ae",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afebe435",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20 \n",
    "EARLY = 3\n",
    "n_slices = 32\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Definir los rangos de LR y WEIGHT\n",
    "LR_RANGE = [1e-3,  8e-4, 5e-4]\n",
    "WEIGHT_RANGE = [1e-3, 8e-4,  5e-4]\n",
    "\n",
    "# Crear todas las combinaciones posibles (16 en total)\n",
    "grid = list(itertools.product(LR_RANGE, WEIGHT_RANGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cda6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✳️ Entrenando modelo Ejex_1_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.001-0.001 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.7584 ||BAC =0.5000 | Recall =1.0000 | AUC=0.4276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 3/20 | TrainLoss=0.6566 ||BAC =0.5086 | Recall =0.5000 | AUC=0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 5\n",
      "Tiempo total de entrenamiento: 5.46 minutos\n",
      "Best model: ../Models_Output/Ejex_1_32_4_20_2_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.6813 | AUC = 0.6813 | recall = 0.8000  | f1 = 0.2667\n",
      "\n",
      "Matriz de confusión:\n",
      "[[27 21]\n",
      " [ 1  4]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.96      0.56      0.71        48\n",
      "    Dementia       0.16      0.80      0.27         5\n",
      "\n",
      "    accuracy                           0.58        53\n",
      "   macro avg       0.56      0.68      0.49        53\n",
      "weighted avg       0.89      0.58      0.67        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_2_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.001-0.0008 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.6799 ||BAC =0.4983 | Recall =0.1000 | AUC=0.4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 4/20 | TrainLoss=0.6740 ||BAC =0.5000 | Recall =0.0000 | AUC=0.4724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 5/20 | TrainLoss=0.6488 ||BAC =0.5155 | Recall =0.1000 | AUC=0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 7\n",
      "Tiempo total de entrenamiento: 7.82 minutos\n",
      "Best model: ../Models_Output/Ejex_2_32_4_20_4_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.4896 | AUC = 0.4896 | recall = 0.0000  | f1 = 0.0000\n",
      "\n",
      "Matriz de confusión:\n",
      "[[47  1]\n",
      " [ 5  0]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.90      0.98      0.94        48\n",
      "    Dementia       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.89        53\n",
      "   macro avg       0.45      0.49      0.47        53\n",
      "weighted avg       0.82      0.89      0.85        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_3_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.001-0.0005 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.6988 ||BAC =0.4431 | Recall =0.3000 | AUC=0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 4/20 | TrainLoss=0.6028 ||BAC =0.4569 | Recall =0.5000 | AUC=0.3483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 5/20 | TrainLoss=0.6525 ||BAC =0.5500 | Recall =0.1000 | AUC=0.5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 7\n",
      "Tiempo total de entrenamiento: 7.94 minutos\n",
      "Best model: ../Models_Output/Ejex_3_32_4_20_4_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.6000 | AUC = 0.6000 | recall = 0.2000  | f1 = 0.3333\n",
      "\n",
      "Matriz de confusión:\n",
      "[[48  0]\n",
      " [ 4  1]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.92      1.00      0.96        48\n",
      "    Dementia       1.00      0.20      0.33         5\n",
      "\n",
      "    accuracy                           0.92        53\n",
      "   macro avg       0.96      0.60      0.65        53\n",
      "weighted avg       0.93      0.92      0.90        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_4_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.0008-0.001 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.7005 ||BAC =0.3966 | Recall =0.0000 | AUC=0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 2/20 | TrainLoss=0.6799 ||BAC =0.5000 | Recall =0.0000 | AUC=0.6517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 4\n",
      "Tiempo total de entrenamiento: 5.13 minutos\n",
      "Best model: ../Models_Output/Ejex_4_32_4_20_1_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.5000 | AUC = 0.5000 | recall = 0.0000  | f1 = 0.0000\n",
      "\n",
      "Matriz de confusión:\n",
      "[[48  0]\n",
      " [ 5  0]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.91      1.00      0.95        48\n",
      "    Dementia       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.91        53\n",
      "   macro avg       0.45      0.50      0.48        53\n",
      "weighted avg       0.82      0.91      0.86        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_5_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.0008-0.0008 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.6815 ||BAC =0.5000 | Recall =1.0000 | AUC=0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 3\n",
      "Tiempo total de entrenamiento: 4.08 minutos\n",
      "Best model: ../Models_Output/Ejex_5_32_4_20_0_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.5000 | AUC = 0.5000 | recall = 1.0000  | f1 = 0.1724\n",
      "\n",
      "Matriz de confusión:\n",
      "[[ 0 48]\n",
      " [ 0  5]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.00      0.00      0.00        48\n",
      "    Dementia       0.09      1.00      0.17         5\n",
      "\n",
      "    accuracy                           0.09        53\n",
      "   macro avg       0.05      0.50      0.09        53\n",
      "weighted avg       0.01      0.09      0.02        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_6_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.0008-0.0005 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.7531 ||BAC =0.4534 | Recall =0.7000 | AUC=0.3931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 2/20 | TrainLoss=0.6452 ||BAC =0.4586 | Recall =0.4000 | AUC=0.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 4/20 | TrainLoss=0.6675 ||BAC =0.4638 | Recall =0.1000 | AUC=0.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 6/20 | TrainLoss=0.5903 ||BAC =0.4828 | Recall =0.0000 | AUC=0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 8/20 | TrainLoss=0.6185 ||BAC =0.5172 | Recall =1.0000 | AUC=0.4966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 9/20 | TrainLoss=0.5820 ||BAC =0.5328 | Recall =0.1000 | AUC=0.4759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 11\n",
      "Tiempo total de entrenamiento: 12.45 minutos\n",
      "Best model: ../Models_Output/Ejex_6_32_4_20_8_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.4479 | AUC = 0.4479 | recall = 0.0000  | f1 = 0.0000\n",
      "\n",
      "Matriz de confusión:\n",
      "[[43  5]\n",
      " [ 5  0]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.90      0.90      0.90        48\n",
      "    Dementia       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.81        53\n",
      "   macro avg       0.45      0.45      0.45        53\n",
      "weighted avg       0.81      0.81      0.81        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_7_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.0005-0.001 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.6919 ||BAC =0.5000 | Recall =0.0000 | AUC=0.5069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Early stopping activado 3\n",
      "Tiempo total de entrenamiento: 4.25 minutos\n",
      "Best model: ../Models_Output/Ejex_7_32_4_20_0_3.pth\n",
      "\n",
      " TEST =====\n",
      "BAC = 0.5000 | AUC = 0.5000 | recall = 0.0000  | f1 = 0.0000\n",
      "\n",
      "Matriz de confusión:\n",
      "[[48  0]\n",
      " [ 5  0]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Dementia       0.91      1.00      0.95        48\n",
      "    Dementia       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.91        53\n",
      "   macro avg       0.45      0.50      0.48        53\n",
      "weighted avg       0.82      0.91      0.86        53\n",
      "\n",
      "✳️ Entrenando modelo Ejex_8_32_4\n",
      "\n",
      "⏺️ Épocas 20 | LR-WD 0.0005-0.0008 | Patience 3\n",
      "▶️ Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Mejor modelo guardadoEpoch 1/20 | TrainLoss=0.6756 ||BAC =0.5000 | Recall =0.0000 | AUC=0.5931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  13%|█▎        | 4/31 [00:06<00:46,  1.73s/it, batch_loss=0.518] "
     ]
    }
   ],
   "source": [
    "# Ejecutar 1\n",
    "\n",
    "BATCH = 4\n",
    "# ---------------------------\n",
    "# Ejecutar cada combinación\n",
    "for i, (LR, WEIGHT) in enumerate(grid, start=1):\n",
    "    MODELO = f\"Ejex_{i}_{n_slices}_{BATCH}\"\n",
    "    main(df, DEVICE, n_slices, EPOCHS, LR, WEIGHT, EARLY, BATCH, MODELO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
